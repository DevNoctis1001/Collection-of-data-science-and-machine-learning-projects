{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Models To Compare Features: Compare And Evaluate All Models\n",
    "\n",
    "In this section, we will do the following:\n",
    "1. Evaluate all of our saved models on the validation set\n",
    "2. Select the best model based on performance on the validation set\n",
    "3. Evaluate that model on the holdout test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age_clean</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>89.1042</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.5000</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.8750</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Sex  Age_clean  SibSp  Parch     Fare  Cabin  Embarked\n",
       "0       1    0  29.699118      1      0  89.1042     86         0\n",
       "1       1    1  45.500000      0      0  28.5000     56         2\n",
       "2       3    1  29.699118      0      0   7.7500    147         1\n",
       "3       2    0  24.000000      1      0  26.0000    147         2\n",
       "4       2    1  36.000000      0      0  12.8750     90         0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from time import time\n",
    "%matplotlib inline\n",
    "\n",
    "val_features_raw = pd.read_csv('../Data/Final_Data/val_features_raw.csv') \n",
    "val_features_original = pd.read_csv('../Data/Final_Data/val_features_original.csv') \n",
    "val_features_all = pd.read_csv('../Data/Final_Data/val_features_all.csv') \n",
    "val_features_reduced = pd.read_csv('../Data/Final_Data/val_features_reduced.csv') \n",
    "\n",
    "val_labels = pd.read_csv('../Data/Final_Data/val_labels.csv')\n",
    "\n",
    "val_features_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read In Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in models\n",
    "models = {}\n",
    "\n",
    "for mdl in ['raw_original', 'cleaned_original', 'all', 'reduced']:\n",
    "    models[mdl] = joblib.load('../Pickled_Models/mdl_{}_features.pkl'.format(mdl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Models On The Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, features, labels):\n",
    "    start = time()\n",
    "    pred = model.predict(features)\n",
    "    end = time()\n",
    "    \n",
    "    accuracy = round(accuracy_score(labels, pred), 3)\n",
    "    precision = round(precision_score(labels, pred), 3)\n",
    "    recall = round(recall_score(labels, pred), 3)\n",
    "    \n",
    "    print('{} \\t-- \\tAccuracy: {} / Precision: {} / Recall: {} / Latency: {}ms'.format(name,\n",
    "                                                                                                                     accuracy,\n",
    "                                                                                                                     precision,\n",
    "                                                                                                                     recall,\n",
    "                                                                                                                     round((end - start) * 1000 , 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Features \t-- \tAccuracy: 0.809 / Precision: 0.782 / Recall: 0.662 / Latency: 54.0ms\n",
      "Cleaned Features \t-- \tAccuracy: 0.803 / Precision: 0.778 / Recall: 0.646 / Latency: 82.1ms\n",
      "All Features \t-- \tAccuracy: 0.831 / Precision: 0.797 / Recall: 0.723 / Latency: 88.1ms\n",
      "Reduced Features \t-- \tAccuracy: 0.809 / Precision: 0.772 / Recall: 0.677 / Latency: 20.0ms\n"
     ]
    }
   ],
   "source": [
    "# Evaluate all of our models on the validation set\n",
    "evaluate_model('Raw Features', models['raw_original'], val_features_raw, val_labels)\n",
    "evaluate_model('Cleaned Features', models['cleaned_original'], val_features_original, val_labels)\n",
    "evaluate_model('All Features', models['all'], val_features_all, val_labels)\n",
    "evaluate_model('Reduced Features', models['reduced'], val_features_reduced, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see `All Features` has the best Accuracy, Precision and Recall. but if there is any requirement for less latency, we would have chosen `Reduced Features` which is the second best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Best Model On Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our test features\n",
    "test_features = pd.read_csv('../Data/Final_Data/test_features_all.csv')\n",
    "test_labels = pd.read_csv('../Data/Final_Data/test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Features \t-- \tAccuracy: 0.816 / Precision: 0.831 / Recall: 0.711 / Latency: 98.1ms\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our final model on the test set\n",
    "evaluate_model('All Features', models['all'], test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
