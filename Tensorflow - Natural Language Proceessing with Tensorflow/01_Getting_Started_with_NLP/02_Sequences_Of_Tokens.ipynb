{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Sequences_Of_Tokens.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2_YdwCOp7Ky"
      },
      "source": [
        "# Creating sequences of tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "TobytpTAibLp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvRxVQ1xc3XR"
      },
      "source": [
        "## Define training sentences in a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9vIbV6tqPsB"
      },
      "source": [
        "##define list of sentences to tokenize\n",
        "train_sentences = [\n",
        "             'It is a sunny day',\n",
        "             'It is a cloudy day',\n",
        "             'Will it rain today?'\n",
        "]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dijfmx8xdZ6F"
      },
      "source": [
        "## Train the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VptkQzDAqkgB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac50f9d-fc20-442f-c657-2c15f127c625"
      },
      "source": [
        "##set up the tokenizer\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "\n",
        "##train the tokenizer on training sentences\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "##store word index for the words in the sentence\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'it': 1, 'is': 2, 'a': 3, 'day': 4, 'sunny': 5, 'cloudy': 6, 'will': 7, 'rain': 8, 'today': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sioLj-JsddVl"
      },
      "source": [
        "## Create sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lcfheKeq2ak"
      },
      "source": [
        "##create sequences using tokenizer\n",
        "sequences = tokenizer.texts_to_sequences(train_sentences)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSmb-p0mrOqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c656b09-8da6-4f0b-fd6f-af3a072d88d4"
      },
      "source": [
        "##print word index dictionary and sequences\n",
        "print('word_index: ', word_index)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_index:  {'it': 1, 'is': 2, 'a': 3, 'day': 4, 'sunny': 5, 'cloudy': 6, 'will': 7, 'rain': 8, 'today': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGY_CHTjrSra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e33e0fa2-26f6-4223-d506-436159c7c2c5"
      },
      "source": [
        "##print sample sentence and sequence\n",
        "print(train_sentences[0])\n",
        "print(sequences[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It is a sunny day\n",
            "[1, 2, 3, 5, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QojDIf_aseIE"
      },
      "source": [
        "## Tokenizing new data using the same tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnFZFUlXsbag"
      },
      "source": [
        "new_sentences = [\n",
        "                 'Will it be raining today?',\n",
        "                 'It is a pleasant day.'\n",
        "]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzWszj9BuDoY"
      },
      "source": [
        "new_sequences = tokenizer.texts_to_sequences(new_sentences)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss1XZ8ttuK-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17cc9039-b75c-4328-b2d0-ec548c75c101"
      },
      "source": [
        "print(new_sentences)\n",
        "print(new_sequences)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Will it be raining today?', 'It is a pleasant day.']\n",
            "[[7, 1, 9], [1, 2, 3, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE: we can see that some of the unseen words are lost when texts are transform to sequences. To overcome this, we can use OOV which is unique word that will be used to replaced for any unseen words.**"
      ],
      "metadata": {
        "id": "qvF5OoiKj-EP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRaRs09MDvbv"
      },
      "source": [
        "## Replacing newly encountered words with special values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cD_c0zLuOAz"
      },
      "source": [
        "##set up the tokenizer again with oov_token\n",
        "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n",
        "\n",
        "##train the new tokenizer on training sentences\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "##store word index for the words in the sentence\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwS1YySvFe2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e857b5b-acaa-4245-db3c-04a8993d1587"
      },
      "source": [
        "##create sequences of the new sentences\n",
        "new_sequences = tokenizer.texts_to_sequences(new_sentences)\n",
        "print(word_index)\n",
        "print(new_sequences)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'it': 2, 'is': 3, 'a': 4, 'day': 5, 'sunny': 6, 'cloudy': 7, 'will': 8, 'rain': 9, 'today': 10}\n",
            "[[8, 2, 1, 1, 10], [2, 3, 4, 1, 5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see that for unseen words in new sentence are replaced to use OOV."
      ],
      "metadata": {
        "id": "hwDQZ1rlqCJp"
      }
    }
  ]
}