{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is divided into two sections:\n",
    "* First, we'll find out what what is necessary to build an NLP system that can turn a body of text into a numerical array of *features* by **manually calcuating frequencies and building out TF-IDF**.\n",
    "* Next we'll show how to perform these steps **using scikit-learn tools**.\n",
    "---------\n",
    "\n",
    "+ [Part One: Core Concepts on Feature Extraction (MANUALLY)](#partone)\n",
    "    + [1) Start with some documents:](#1)\n",
    "    + [2) Building a vocabulary (Creating a \"Bag of Words\")](#2)\n",
    "        + [2.1) Getting the unique words only](#2.1)\n",
    "        + [2.2) Get all unique words across all documents (both One and Two)](#2.2)\n",
    "        + [2.3) Create vocab dictionary with related index](#2.3)\n",
    "    + [3)Bag of Words to Frequency Counts](#3)\n",
    "        + [3.1) Make A list of All Vocab (which will be used to map later)](#3.1)\n",
    "        + [3.2) Add in counts per word per doc:](#3.2)\n",
    "        + [3.3) Create the DataFrame:](#3.3)\n",
    "---------\n",
    "+ [Concepts to Consider:](#concept)\n",
    "    + [Bag of Words and Tf-idf](#bow)\n",
    "    + [Stop Words and Word Stems](#stopwordswordstems)\n",
    "    + [Tokenization and Tagging](#tokenizationtagging)\n",
    "--------\n",
    "\n",
    "+ [Part Two:  Feature Extraction with Scikit-Learn](#part2)\n",
    "    + [1) CountVectorizer](#countvect)\n",
    "    + [`stop_words` parameter](#stopwords)\n",
    "    + [2) TfidfTransformer](#tfidf)\n",
    "    + [3) Using Pipeline (combining two steps of CountVectorizer + Tfidf Transformer)](#pipeline)\n",
    "    + [4) TfIdfVectorizer (same as step 1 + step 2)](#tfidfvector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=partone>Part One: Core Concepts on Feature Extraction (MANUALLY)</a>\n",
    "\n",
    "In this section we'll use basic Python to build a rudimentary NLP system. We'll build a *corpus of documents* (two small text files), create a *vocabulary* from all the words in both documents, and then demonstrate a *Bag of Words* technique to extract features from each document.<br>\n",
    "<div class=\"alert alert-info\" style=\"margin: 20px\">This first section is for illustration only!\n",
    "<br>Don't worry about memorizing this code - later on we will let Scikit-Learn Preprocessing tools do this for us.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=1>1) Start with some documents:</a>\n",
    "For simplicity we won't use any punctuation in the text files One.txt and Two.txt. Let's quickly open them and read them. Keep in mind, you should avoid opening and reading entire files if they are very large, as Python could just display everything depending on how you open the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a story about dogs\n",
      "our canine pets\n",
      "Dogs are furry animals\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../Data/One.txt') as mytext:\n",
    "    a = mytext.read()\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a story about dogs\\n', 'our canine pets\\n', 'Dogs are furry animals\\n']\n"
     ]
    }
   ],
   "source": [
    "# readlines returns as list\n",
    "with open('../Data/One.txt') as mytext:\n",
    "    a = mytext.readlines()\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading entire text as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/Two.txt') as mytext:\n",
    "    entire_text = mytext.read()\n",
    "    entire_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This story is about surfing\n",
      "Catching waves is fun\n",
      "Surfing is a popular water sport\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(entire_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Each Line as a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/One.txt') as mytext:\n",
    "    lines = mytext.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a story about dogs\\n',\n",
       " 'our canine pets\\n',\n",
       " 'Dogs are furry animals\\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Words Separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/One.txt') as mytext:\n",
    "    words = mytext.read().lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'story',\n",
       " 'about',\n",
       " 'dogs',\n",
       " 'our',\n",
       " 'canine',\n",
       " 'pets',\n",
       " 'dogs',\n",
       " 'are',\n",
       " 'furry',\n",
       " 'animals']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=2>2) Building a vocabulary (Creating a \"Bag of Words\")</a>\n",
    "\n",
    "Let's create dictionaries that correspond to **unique mappings of the words in the documents**. We can begin to think of this as mapping out all the possible words available for all (both) documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in One.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/One.txt') as mytext:\n",
    "    words_one = mytext.read().lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'story',\n",
       " 'about',\n",
       " 'dogs',\n",
       " 'our',\n",
       " 'canine',\n",
       " 'pets',\n",
       " 'dogs',\n",
       " 'are',\n",
       " 'furry',\n",
       " 'animals']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=2.1>2.1) Getting the unique words only</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_one = set(words_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'animals',\n",
       " 'are',\n",
       " 'canine',\n",
       " 'dogs',\n",
       " 'furry',\n",
       " 'is',\n",
       " 'our',\n",
       " 'pets',\n",
       " 'story',\n",
       " 'this'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_words_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only have 12 unique words instead of original 13 words in Document one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat for Two.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/Two.txt') as mytext:\n",
    "    words_two = mytext.read().lower().split()\n",
    "    unique_words_two = set(words_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 12)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_two), len(unique_words_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=2.2>2.2) Get all unique words across all documents (both One and Two)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique_words = set()\n",
    "\n",
    "all_unique_words.update(unique_words_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'our', 'canine', 'about', 'pets', 'a', 'this', 'are', 'furry', 'animals', 'dogs', 'is', 'story'}\n"
     ]
    }
   ],
   "source": [
    "print(all_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique_words.update(unique_words_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'water', 'catching', 'waves', 'this', 'are', 'furry', 'animals', 'sport', 'is', 'story', 'our', 'canine', 'about', 'pets', 'a', 'popular', 'fun', 'dogs', 'surfing'}\n"
     ]
    }
   ],
   "source": [
    "print(all_unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=2.3>2.3) Create vocab dictionary with related index</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocab = {}\n",
    "i = 0\n",
    "\n",
    "for word in all_unique_words:\n",
    "    full_vocab[word] = i\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note that Set is not ordered. So words will not in ordered.\n",
    "The for loop goes through the set() in the most efficient way possible, not in alphabetical order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'water': 0,\n",
       " 'catching': 1,\n",
       " 'waves': 2,\n",
       " 'this': 3,\n",
       " 'are': 4,\n",
       " 'furry': 5,\n",
       " 'animals': 6,\n",
       " 'sport': 7,\n",
       " 'is': 8,\n",
       " 'story': 9,\n",
       " 'our': 10,\n",
       " 'canine': 11,\n",
       " 'about': 12,\n",
       " 'pets': 13,\n",
       " 'a': 14,\n",
       " 'popular': 15,\n",
       " 'fun': 16,\n",
       " 'dogs': 17,\n",
       " 'surfing': 18}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=3>3)Bag of Words to Frequency Counts</a>\n",
    "\n",
    "Now that we've encapsulated our \"entire language\" in a dictionary, let's perform *feature extraction* on each of our original documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empty counts per doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_freq = [0] * len(full_vocab)\n",
    "two_freq = [0] * len(full_vocab)\n",
    "all_words = [''] * len(full_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=3.1>3.1) Make A list of All Vocab (which will be used to map later)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in full_vocab:\n",
    "    word_index = full_vocab[word]\n",
    "    all_words[word_index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['water', 'catching', 'waves', 'this', 'are', 'furry', 'animals', 'sport', 'is', 'story', 'our', 'canine', 'about', 'pets', 'a', 'popular', 'fun', 'dogs', 'surfing']\n"
     ]
    }
   ],
   "source": [
    "print(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=3.2>3.2) Add in counts per word per doc:</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the frequencies of each word in 1.txt to our vector:\n",
    "with open('../Data/One.txt') as file:\n",
    "    one_text  = file.read().lower().split()\n",
    "    \n",
    "for word in one_text:\n",
    "    word_index = full_vocab[word] #get the index of that specific word\n",
    "    one_freq[word_index] += 1 # increase by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 0]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the second document:\n",
    "with open('../Data/Two.txt') as file:\n",
    "    two_text = file.read().lower().split()\n",
    "    \n",
    "for word in two_text:\n",
    "    word_index = full_vocab[word]\n",
    "    two_freq[word_index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 0, 0, 0, 1, 3, 1, 0, 0, 1, 0, 1, 1, 1, 0, 2]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=3.3>3.3) Create the DataFrame:</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = pd.DataFrame(data=[one_freq, two_freq], columns=all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>water</th>\n",
       "      <th>catching</th>\n",
       "      <th>waves</th>\n",
       "      <th>this</th>\n",
       "      <th>are</th>\n",
       "      <th>furry</th>\n",
       "      <th>animals</th>\n",
       "      <th>sport</th>\n",
       "      <th>is</th>\n",
       "      <th>story</th>\n",
       "      <th>our</th>\n",
       "      <th>canine</th>\n",
       "      <th>about</th>\n",
       "      <th>pets</th>\n",
       "      <th>a</th>\n",
       "      <th>popular</th>\n",
       "      <th>fun</th>\n",
       "      <th>dogs</th>\n",
       "      <th>surfing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   water  catching  waves  this  are  furry  animals  sport  is  story  our  \\\n",
       "0      0         0      0     1    1      1        1      0   1      1    1   \n",
       "1      1         1      1     1    0      0        0      1   3      1    0   \n",
       "\n",
       "   canine  about  pets  a  popular  fun  dogs  surfing  \n",
       "0       1      1     1  1        0    0     2        0  \n",
       "1       0      1     0  1        1    1     0        2  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can how frequently each word appears in the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the vectors we see that some words are common to both, some appear only in `One.txt`, others only in `Two.txt`. Extending this logic to tens of thousands of documents, we would see the vocabulary dictionary grow to hundreds of thousands of words. Vectors would contain mostly zero values, making them **sparse matrices**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=concept>Concepts to Consider:</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=bow>Bag of Words and Tf-idf</a>\n",
    "In the above examples, each vector can be considered a *bag of words*. By itself these may not be helpful until we consider *term frequencies*, or how often individual words appear in documents. A simple way to calculate term frequencies is to divide the number of occurrences of a word by the total number of words in the document. In this way, the number of times a word appears in large documents can be compared to that of smaller documents.\n",
    "\n",
    "However, it may be hard to differentiate documents based on term frequency if a word shows up in a majority of documents. To handle this we also consider *inverse document frequency*, which is the total number of documents divided by the number of documents that contain the word. In practice we convert this value to a logarithmic scale, as described [here](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency).\n",
    "\n",
    "Together these terms become [**tf-idf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=stopwordswordstems>Stop Words and Word Stems</a>\n",
    "\n",
    "Some words like \"the\" and \"and\" appear so frequently, and in so many documents, that we needn't bother counting them. Also, it may make sense to only record the root of a word, say `cat` in place of both `cat` and `cats`. This will shrink our vocab array and improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=tokenizationtagging>Tokenization and Tagging</a>\n",
    "When we created our vectors the first thing we did was split the incoming text on whitespace with `.split()`. This was a crude form of *tokenization* - that is, dividing a document into individual words. In this simple example we didn't worry about punctuation or different parts of speech. In the real world we rely on some fairly sophisticated *morphology* to parse text appropriately.\n",
    "\n",
    "Once the text is divided, we can go back and *tag* our tokens with information about parts of speech, grammatical dependencies, etc. This adds more dimensions to our data and enables a deeper understanding of the context of specific documents. For this reason, vectors become ***high dimensional sparse matrices***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=part2>Part Two:  Feature Extraction with Scikit-Learn</a>\n",
    "\n",
    "Let's explore the more realistic process of using sklearn to complete the tasks mentioned above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn's Text Feature Extraction Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    'This is a line',\n",
    "    'This is another line',\n",
    "    'Completely different line',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=countvect>1) CountVectorizer</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ cv will treat each value as one document\n",
    "+ `fit_transform` is basically doing get the unique vocabulary on fit and then transform it by acutally performing frequency count on each documents insides that list.\n",
    "+ and it returns the `sparse matrix`. The reasonis when performing vectorizing and building out bag of words model, what's going to happen is most of the items in the matrix are going to be zeros. So when you are dealing with hundards and thousands of documents with many many words, you want to make sure you do't eat up too much of PC's memory unnecessarily by just storing a bunch of zeros. Which is why we have sparse matrix.\n",
    "+ sparse matrix with 3x6 matrix. Why 3? because there are 3 documents in the list we passed. When we used `todense()` method, we can see the originally stored frequency count which is not in sparse matrix form (which stores information in memory efficient way). **NOTE: we don't want to call this method if we have a large values of words, which gonna take a lot of memory space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using todense() to see in original form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 1, 1, 1],\n",
       "        [1, 0, 0, 1, 1, 1],\n",
       "        [0, 1, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 5, 'is': 3, 'line': 4, 'another': 0, 'completely': 1, 'different': 2}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we closely look at the value, `another` is at index `0`. So if you look at results of todense(), see index 0 has value 1 on second document.\n",
    "\n",
    "which make senese because  **This is another line** is the second document which has the word another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=stopwords>`stop_words` parameter</a>\n",
    "+ with the use of this parameter, common stop words in English are not longer part of the vocab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'line': 2, 'completely': 0, 'different': 1}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=tfidf>2) TfidfTransformer</a>\n",
    "\n",
    "+ TfidfVectorizer is used on sentences, while TfidfTransformer is used on an existing count matrix, such as one returned by CountVectorizer\n",
    "+ using this, **we can transform Bag of Words into TF-IDF**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tfidf.fit_transform(sparse_matrix) # BOW ===> TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.61980538, 0.48133417,\n",
       "         0.61980538],\n",
       "        [0.63174505, 0.        , 0.        , 0.4804584 , 0.37311881,\n",
       "         0.4804584 ],\n",
       "        [0.        , 0.65249088, 0.65249088, 0.        , 0.38537163,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=pipeline>3) Using Pipeline (combining two steps of CountVectorizer + Tfidf Transformer)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pipe.fit_transform(text)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.61980538, 0.48133417,\n",
       "         0.61980538],\n",
       "        [0.63174505, 0.        , 0.        , 0.4804584 , 0.37311881,\n",
       "         0.4804584 ],\n",
       "        [0.        , 0.65249088, 0.65249088, 0.        , 0.38537163,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=tfidfvector>4) TfIdfVectorizer</a>\n",
    "\n",
    "Does both above (step 1 and 2) in a single step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_results = tv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.61980538, 0.48133417,\n",
       "         0.61980538],\n",
       "        [0.63174505, 0.        , 0.        , 0.4804584 , 0.37311881,\n",
       "         0.4804584 ],\n",
       "        [0.        , 0.65249088, 0.65249088, 0.        , 0.38537163,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_results.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see both method yield same results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-datascience",
   "language": "python",
   "name": "venv-datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
