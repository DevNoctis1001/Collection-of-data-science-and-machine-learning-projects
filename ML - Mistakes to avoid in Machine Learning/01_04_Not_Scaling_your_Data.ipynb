{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ws1s9PkH2vJ3"
   },
   "source": [
    "# **Why do we need to scale features?**\n",
    "\n",
    "* Many machine learning techniques will incorrectly assign a **higher weight to features of a higher magnitude**.\n",
    "\n",
    "* There are wo common approaches for scaling.\n",
    "    + **MinMaxScaler**\n",
    "    + **StandardScaler**\n",
    "\n",
    "\n",
    "**NOTE: tree based algorithms don't require Scaling**\n",
    "\n",
    "-------\n",
    "\n",
    "## Min Max Scaler\n",
    "\n",
    "+ Min-max scaling involves scaling your feature to `a range between 0 and 1`, as defined by the `min and max of your feature`.\n",
    "+ is recommended when your algorithm doesn't require assumptions about the distributions of your variables, as in the case of KNN.\n",
    "\n",
    "\n",
    "## Standard Scaler\n",
    "\n",
    "+ The StandardScaler will scale features to be the `standard deviation from the mean for that feature`. Thus we have a `range of values both positive and negative`.\n",
    "+ this approach assumes a bell curve distribution (Normal Distribution) for your variables and it's most effective when it's the case.\n",
    "\n",
    "\n",
    "-------\n",
    "\n",
    "### Before we scale, we need to perform the train-test split.\n",
    "\n",
    "The reason we do scaling is that we will actually derive the scaling bounds from the training set, then apply it to test set.\n",
    "\n",
    "Remember in machine learning, it's important that anything our model learns must come from training set, not the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.8</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.6</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.9</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.8</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38           122.8     1001.0          0.11840   \n",
       "1        20.57         17.77           132.9     1326.0          0.08474   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33            184.6      2019.0   \n",
       "1                 0.05667  ...          23.41            158.8      1956.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "\n",
       "[2 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])\n",
    "df['target'] = pd.Series(cancer['target'])\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tO__cBqQ6NQM"
   },
   "source": [
    "# Train, Test, Split\n",
    "\n",
    "### Before we scale, we need to perform the train-test split.\n",
    "\n",
    "The reason we do scaling is that we will actually derive the scaling bounds from the training set, then apply it to test set.\n",
    "\n",
    "Remember in machine learning, it's important that anything our model learns must come from training set, not the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('target', axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x1LOTFie4drG"
   },
   "source": [
    "# Two common approaches: **MinMaxScaler & StandardScaler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min Max Scaler\n",
    "\n",
    "+ Min-max scaling involves scaling your feature to `a range between 0 and 1`, as defined by the `min and max of your feature`.\n",
    "+ is recommended when your algorithm doesn't require assumptions about the distributions of your variables, as in the case of KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "text",
    "id": "pH2sUq0E8Nt2"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = scaler.fit_transform(X_train) \n",
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21340338, 0.20248963, 0.20869325, ..., 0.25597658, 0.2712399 ,\n",
       "        0.24111242],\n",
       "       [0.16607506, 0.36929461, 0.15942229, ..., 0.22487082, 0.12773507,\n",
       "        0.1533517 ],\n",
       "       [0.2493729 , 0.34149378, 0.23826964, ..., 0.28284533, 0.30514488,\n",
       "        0.17237308],\n",
       "       ...,\n",
       "       [0.11619102, 0.35726141, 0.11077327, ..., 0.17402687, 0.17524147,\n",
       "        0.17263545],\n",
       "       [0.12963226, 0.35311203, 0.11706171, ..., 0.        , 0.06780997,\n",
       "        0.06919848],\n",
       "       [0.21434995, 0.59004149, 0.21235575, ..., 0.33251808, 0.10782574,\n",
       "        0.21172767]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scaler\n",
    "\n",
    "+ The StandardScaler will scale features to be the `standard deviation from the mean for that feature`. Thus we have a `range of values both positive and negative`.\n",
    "+ this approach assumes a bell curve distribution (Normal Distribution) for your variables and it's most effective when it's the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q3K8PXgGGTIz"
   },
   "outputs": [],
   "source": [
    "scaled_X_train = scaler.fit_transform(X_train) \n",
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.74998027, -1.09978744, -0.74158608, ..., -0.6235968 ,\n",
       "         0.07754241,  0.45062841],\n",
       "       [-1.02821446, -0.1392617 , -1.02980434, ..., -0.7612376 ,\n",
       "        -1.07145262, -0.29541379],\n",
       "       [-0.53852228, -0.29934933, -0.56857428, ..., -0.50470441,\n",
       "         0.34900827, -0.13371556],\n",
       "       ...,\n",
       "       [-1.3214733 , -0.20855336, -1.3143845 , ..., -0.98621857,\n",
       "        -0.69108476, -0.13148524],\n",
       "       [-1.24245479, -0.23244704, -1.27759928, ..., -1.7562754 ,\n",
       "        -1.55125275, -1.01078909],\n",
       "       [-0.74441558,  1.13188181, -0.72016173, ..., -0.28490593,\n",
       "        -1.2308599 ,  0.20083251]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_train"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN4QBUPjQvU23gCfftcQ2At",
   "name": "04 - Not Scaling your Data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
