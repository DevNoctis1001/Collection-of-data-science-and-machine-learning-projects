{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Overview of Face Recognition\n",
    "\n",
    "## What can you do with Face Recognition\n",
    "\n",
    "- Identity Verification (use instead of card system or log in system)\n",
    "- quickly sort through large datasets (let's say for News Agency: instead of manually labelling the photos, system can automatically recognize some top celebrities present on those photos and tag them accordingly) \n",
    "- Security Survalliance (for police department: can search for Person of interest across hundreds of camers, photos, video clips)\n",
    "- can detect new person appears (example: can count number of people in photos or video feeds, Some billboards install cameras and count the number of people who look and check the number to measure sucess of advertisement)\n",
    "- can detect how smiliar are two people (example: you can provide your photos and can check who is your celebrity doppelganger)\n",
    "\n",
    "![img/1.png](img/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Tools for Facial Recognition\n",
    "Commerical Face Recognition Services\n",
    "- Amazon Rekogition API\n",
    "- Microsoft Azure Face API\n",
    "\n",
    "Open-Source Face Recognition Tools\n",
    "- OpenFace\n",
    "- dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "# Face Recognition as Multi-step Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![img/2.png](img/2.png)\n",
    "![img/3.png](img/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2\n",
    "- we need to take into account of head position. Otherwise system will think the same person as two different people.\n",
    "- so we use ML algorithms to detect the facial features. We look for the position of eyes, nose and mouth.\n",
    "- Then we pass those positions to another step.\n",
    "![img/4.png](img/4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/5.png](img/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using the position template and align the face, we will get the following.\n",
    "\n",
    "![img/6.png](img/6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/7.png](img/7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/8.png](img/8.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Euclidean formula basically measure how far betwen those images.\n",
    "![img/9.png](img/9.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/10.png](img/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# 3) Pipeline Step 1) :Face Detection\n",
    "\n",
    "## What is face detection?\n",
    "The ability to detect and locate human faces in photograph.\n",
    "\n",
    "![img/31.png](img/31.png)\n",
    "![img/32.png](img/32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection Algorithms\n",
    "![img/33.png](img/33.png)\n",
    "\n",
    "The below algorithm is not used anymore. But it is suitable for devices with very low power.\n",
    "![img/34.png](img/34.png)\n",
    "![img/35.png](img/35.png)\n",
    "![img/36.png](img/36.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Analyzing an image as a histogram of oriented gradients\n",
    "\n",
    "Let's take a look a the histogram of oriented gradients, or HOG, algorithm, and see how it is used to look for faces in photographs. Here we have an image that we wanna check for faces.\n",
    "![img/hog.png](img/hog.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to convert the input image to black-and-white. The HOG algorithm only looks at changes between light and dark areas in an image. It doesn't need color information, so we can throw it away.\n",
    "![img/hog1.png](img/hog1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll look at every single pixel in the image, one at a time. As an example, let's zoom in on one small area of this image, so we can see each pixel clearly. Let's look at this pixel right here as an example.\n",
    "![img/hog2.png](img/hog2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll look at the pixels directly next to this pixel. Here, I've highlighted the pixels above, below, and on each side.\n",
    "![img/hog3.png](img/hog3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to measure how dark this pixel is compared to the pixels surrounding it, and we wanna find the direction where the biggest change happens. In this case, we can see that the pixel to the left is much lighter than this pixel, and the pixel to the right is darker than this pixel. In other words, at this exact point, the image is transitioning from a light area to a darker area. Based on that, we'll draw an arrow on top of this pixel that points from left to right. This shows the movement of lighting at this exact point.\n",
    "![img/hog4.png](img/hog4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we repeat this process for every single pixel in the image, the image turns into a map of transitions from light to dark areas. These lines are called gradients. Each gradient shows how the image flows from a light area to a dark area at that point.\n",
    "![img/hog5.png](img/hog5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now let's zoom back out. Here is what the full gradient map looks like after replacing every pixel. The gradient map is a simplified version of the original image, but it's still pretty complex. Capturing the gradient for every single pixel is more detail than we need. To detect faces, all we really need is the overall structure of the image. In other words, we can simplify this representation further.\n",
    " \n",
    " Let's take a look at one 16x16 pixel area as an example. Inside the square, on the left, we'll count up how many gradients point in each major direction. How many point up, how many point up right, how many point right, and so on. Instead of keeping track of the 256 separate gradients within this box, we'll just store a count of how many gradients point in each direction.\n",
    " \n",
    "![img/hog6.png](img/hog6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this square, the majority of gradients go from top to bottom, so that's the strongest factor that represents this area of the image. There are also gradients pointing in other directions that we'll keep track of. We'll represent those other directions here as lines that are less bold. Now, we can repeat this process for the entire image.\n",
    "\n",
    "![img/hog7.png](img/hog7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end result. The original image is now a simple representation that captures the basic structure. We can use this simplified representation to easily train a face detection model.\n",
    "\n",
    "![img/hog8.png](img/hog8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Faces in image with HOG features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn how HOG can help us build a face detection model. To train a face detection model, we'll start by collecting lots of images of different faces and converting them the HOG representations like this. These faces will be our training data.\n",
    "\n",
    "![img/hog10.png](img/hog10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use this HOG representation to train the machine learning model. We'll train the model by giving it lots of examples of HOG representations of faces so it can learn what this pattern looks like. Because faces look very obvious in HOG representations, it's pretty easy to train the machine learning algorithm to recognize those patterns. HOG face detectors can perform well with a fairly small amount of training data. Once the model is trained to recognize these kinds of face patterns, we can use it to find faces in other images.\n",
    "\n",
    "![img/hog11.png](img/hog11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's detect faces in this image. First, we'll convert this image into a HOG representation. Next, we'll use our HOG face detection model as a sliding window detector. We'll slide our face detector over each section of the image and check if it contains a face. Any place that returns true is a part of the image that contains a face.\n",
    " \n",
    "![img/hog12.png](img/hog12.png)\n",
    "![img/hog13.png](img/hog13.png)\n",
    "![img/hog14.png](img/hog14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use HOG?\n",
    "\n",
    "So why is it more effective to train the face detection model using HOG features instead of working directly with the raw images? \n",
    "\n",
    "First, HOG simplifies the image in a way that still retains the key information needed to spot faces. By simplifying the problem this way, it makes it easier for the machine learning model to solve it. But HOG has some other nice advantages, as well, that make it work better for small training sets. \n",
    "\n",
    "First, the HOG representation of an image doesn't change even when you lighten or darken the image. Since HOG only looks for changes in brightness and not absolute brightness, making an image a little brighter or a little darker doesn't change the HOG representation at all. \n",
    "\n",
    "Second, the HOG representation of an image doesn't change even if you change the shapes in the image a little bit. Because it is only looking at broad changes in the intents the over large areas of the image, small changes in shape don't matter. This is great for face detection because it means that two faces that don't look exactly the same will still have nearly the same HOG representation. \n",
    "\n",
    "\n",
    "![img/whyusehog.png](img/whyusehog.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why HOG is resistant to lighting changes?\n",
    "\n",
    "Let's look at an example. Here are two versions of the same photo. These photos are exactly the same except for one copy is brighter than the other. If we analyze the pixel values directly, these two images will have completely different pixel values. To a machine learning algorithm, they will look like two totally different images but the HOG representations for these two images are exactly the same. That means that the machine learning model needs less training data to recognize both of these images correctly.\n",
    "\n",
    "\n",
    "![img/whyusehog2.png](img/whyusehog2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Coding Time:\n",
    "\n",
    "[FaceDetetion Coding](03_Coding_Face_Detection.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Facial Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Step 2) : What is Face Landmark Estimation?\n",
    "\n",
    "Finding the location of key points on a face, such as tip of the nose and center of each eye."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The second step of our face recognition pipeline is called face landmark estimation. Face landmark estimation is where we identify key points on a face, such as the tip of the nose and the center of the eye. On the left is a face that we extracted from a photograph using face detection model. On the right is what that image looks like after we use face landmark estimation to detect points on the face. \n",
    "\n",
    "Each of the points we've detected on the face is called a face landmark because it represents a well-known place on the face like the tip of the nose or the edge of the eyes. Face landmark estimation works by starting with a known set of points that should appear on any face. Then it moves those points around until they match the face image. In this case, we've located the eyebrows, eyes, nose, lips, and chin line. And because the points are predefined, we should already know which of the points make up the eyes, which make up the nose, and so on, so we can connect them with a line.\n",
    "\n",
    "\n",
    "![img/FLE1.png](img/FLE1.png)\n",
    "\n",
    "This particular landmark estimation model is called a 68 point face landmark model because it looks for 68 specific landmarks on each face.\n",
    "\n",
    "![img/FLE2.png](img/FLE2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make face recognition systems run a little more quickly, we can also use a face landmark model with fewer points. Here's an example of a five point model. It only detects the edges of each eye and the bottom of the nose. For some applications, five points might be enough information to get the job done. And since it has to detect fewer points, it can run more quickly.\n",
    "\n",
    "![img/FLE3.png](img/FLE3.png)\n",
    "\n",
    " Face landmark estimation also has many uses outside of facial recognition. If you've ever used a social media app that lets you add funny hats or makeup to your pictures, you've seen a version of face landmark estimation in action. These applications work by first detecting the face landmarks, and then using those points to overlay clothes or makeup in the right place. But the main use for face landmark estimation is called face alignment where we correct for head rotation when doing face recognition.\n",
    "\n",
    "![img/FLE4.png](img/FLE4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-------\n",
    "\n",
    "## Identifying face landmarks with a machine learning model\n",
    "\n",
    "how a machine learning algorithm can be used to identify face landmarks. If you ask a person to place face landmarks on a picture they'll probably do it one at a time, like this. We'll put this point on the edge of the left eye and this point on the edge of the right eye and the point on the tip of the nose and so on. But trying to train the computer to match up each point, one by one, is very difficult. Instead, we can use some tricks to make this problem easier the model.\n",
    "\n",
    "![img/IFLE1.png](img/IFLE1.png)\n",
    "\n",
    "\n",
    "First, we'll assume that all human faces are roughly the same shape. Because of this assumption, all faces must be pretty close to our default face. So instead of trying to match up each point one at a time, we'll just overlay the entire face template on the face and then we'll only ask the computer to move and adjust the template so that each point is closer to the right point. \n",
    "\n",
    "![img/IFLE2.png](img/IFLE2.png)\n",
    "![img/IFLE3.png](img/IFLE3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next trick is that we'll add the constraint of how much the computer can move each point. The rule is that no single point can be moved too far from its neighboring points. Let's move these points into place. Notice that they all move a little but no point moves too far from its neighbor points. This constraint keeps the computer from coming up with face landmarks that are totally wrong. It makes the job easier for the computer since it doesn't have to learn how to find each point one of the time. It just has to learn how to push the points in the right direction.\n",
    "\n",
    "![img/IFLE4.png](img/IFLE4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The third trick we'll use is to split the job of completely fitting the face template into the face into smaller problems. In other words, we'll train several different machine learning models that each do part of the job. \n",
    " \n",
    " The first model is allowed to move the points a good bit but it doesn't have to get everything perfect. It just has to improve the fit over the starting point. Then we'll train the second model, that takes the result of this, and tries to fine tune each point to get them a little bit closer to the right place. It will be allowed to move each point less than the first model. \n",
    " \n",
    " The second model only has to learn how to fix the mistakes of the first model. It doesn't have to learn the complex task of identifying face landmarks from scratch. This makes its job easier. This process of feeding one model's results into another model for fine tuning continues as many as 10 times. \n",
    " \n",
    " The end result of all 10 models working together is that the points end up in exactly the right place but each model only had to be able to do one small job. And once this cascade to face landmark model is trained, it should work for pretty much any face. \n",
    " \n",
    " So in our code, we won't have to train our own model from scratch. We can just use a standard pre-trained model and it should work for all of our images.\n",
    " \n",
    "![img/IFLE5.png](img/IFLE5.png)\n",
    "\n",
    "But it's still important to understand how the model works because it will help you understand its limitations. \n",
    "\n",
    "For example, this model won't be able to identify face landmarks on cartoon faces because it only knows how to match human faces against a human face template. If you wanted to identify face landmarks in cartoon faces, you have to build a new model from scratch using cartoon faces as training data.\n",
    "![img/IFLE6.png](img/IFLE6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posting Faces based on face landmarks\n",
    "\n",
    "## Face Alignment:\n",
    "Adjusting a raw face images so that key facial features (like the eyes, nose and mouth) line up with a predefined template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of face alignment. On the left is a face that was extracted directly from a real photo. Notice how the left side of the head is turned away from the camera and how the face is slightly tilted down. On the right is the aligned version of the face. We've adjusted the image so that the eyes and bottom of the nose are as close to the center as we can get them without causing too much distortion.\n",
    "\n",
    "![img/FA1.png](img/FA1.png)\n",
    "\n",
    "### So, why do we need to align our face images? \n",
    "\n",
    "We want our face recognition system to work even if the person isn't looking directly into the camera. When we take pictures in the real world, a person's head will usually be rotated and not looking directly at the camera. If we don't correct for this head rotation, it will be harder for our face recognition system to recognize people correctly. The system will have to understand that the same face rotated in different ways is the same person.\n",
    "\n",
    "By correcting for head rotation, our face recognition system will have to do a little less work to recognize people and thus will be more accurate over all. \n",
    "\n",
    "![img/FA2.png](img/FA2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's walk through the face alignment process. Here is our original face image.\n",
    " \n",
    "The first step is to detect face landmarks. In this example, we've detected five key points: the sides of each eye and the center point under the nose. Now that we know where the eyes and nose are, we need to figure out how to adjust the image so that the points in our image line up with the template on the right. \n",
    "\n",
    "But we don't want to distort the image too much because it would change the look of the face and throw off the face recognition system's accuracy. One simple solution is to calculate what's called an affine transformation. \n",
    "\n",
    "### Affine Transformation\n",
    "An affine transformation is a linear mapping between set of points where parallel lines remain parallel.\n",
    "![img/FA3.png](img/FA3.png)\n",
    "\n",
    "\n",
    "Basically, this means that we can move, rotate, and stretch our image, but we can't do more complex things like twisting or warping the image. \n",
    "\n",
    "To calculate the affine transformation, we look at the position of the points on the left and figure out what amount of stretching and rotation would get them closest to the points on the right. Then we just apply that same math to the pixels in our image. \n",
    "\n",
    "When we are writing the code in our face recognition system, we won't have to write any code to do this face alignment. The face recognition library we are using will do the face alignment stuff for us. But it's important to know what is happening behind the scenes. \n",
    "\n",
    "\n",
    "If you've run into a case where a face recognition system isn't working properly for a certain image, a good way to debug it is to run the image through your face recognition detection and face landmark detection steps and see what results you get. If you can't detect all five face landmarks in your image, the face alignment step won't work which will prevent the face recognition from working correctly.\n",
    "\n",
    "![img/FA4.png](img/FA4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-datascience",
   "language": "python",
   "name": "venv-datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
