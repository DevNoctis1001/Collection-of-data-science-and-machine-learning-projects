{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Overview of Face Recognition\n",
    "\n",
    "## What can you do with Face Recognition\n",
    "\n",
    "- Identity Verification (use instead of card system or log in system)\n",
    "- quickly sort through large datasets (let's say for News Agency: instead of manually labelling the photos, system can automatically recognize some top celebrities present on those photos and tag them accordingly) \n",
    "- Security Survalliance (for police department: can search for Person of interest across hundreds of camers, photos, video clips)\n",
    "- can detect new person appears (example: can count number of people in photos or video feeds, Some billboards install cameras and count the number of people who look and check the number to measure sucess of advertisement)\n",
    "- can detect how smiliar are two people (example: you can provide your photos and can check who is your celebrity doppelganger)\n",
    "\n",
    "![img/1.png](img/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Tools for Facial Recognition\n",
    "Commerical Face Recognition Services\n",
    "- Amazon Rekogition API\n",
    "- Microsoft Azure Face API\n",
    "\n",
    "Open-Source Face Recognition Tools\n",
    "- OpenFace\n",
    "- dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "# Face Recognition as Multi-step Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![img/2.png](img/2.png)\n",
    "![img/3.png](img/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2\n",
    "- we need to take into account of head position. Otherwise system will think the same person as two different people.\n",
    "- so we use ML algorithms to detect the facial features. We look for the position of eyes, nose and mouth.\n",
    "- Then we pass those positions to another step.\n",
    "![img/4.png](img/4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/5.png](img/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using the position template and align the face, we will get the following.\n",
    "\n",
    "![img/6.png](img/6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/7.png](img/7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/8.png](img/8.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Euclidean formula basically measure how far betwen those images.\n",
    "![img/9.png](img/9.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/10.png](img/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# 3) Face Detection\n",
    "\n",
    "## What is face detection?\n",
    "The ability to detect and locate human faces in photograph.\n",
    "\n",
    "![img/31.png](img/31.png)\n",
    "![img/32.png](img/32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection Algorithms\n",
    "![img/33.png](img/33.png)\n",
    "\n",
    "The below algorithm is not used anymore. But it is suitable for devices with very low power.\n",
    "![img/34.png](img/34.png)\n",
    "![img/35.png](img/35.png)\n",
    "![img/36.png](img/36.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Analyzing an image as a histogram of oriented gradients\n",
    "\n",
    "Let's take a look a the histogram of oriented gradients, or HOG, algorithm, and see how it is used to look for faces in photographs. Here we have an image that we wanna check for faces.\n",
    "![img/hog.png](img/hog.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to convert the input image to black-and-white. The HOG algorithm only looks at changes between light and dark areas in an image. It doesn't need color information, so we can throw it away.\n",
    "![img/hog1.png](img/hog1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll look at every single pixel in the image, one at a time. As an example, let's zoom in on one small area of this image, so we can see each pixel clearly. Let's look at this pixel right here as an example.\n",
    "![img/hog2.png](img/hog2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll look at the pixels directly next to this pixel. Here, I've highlighted the pixels above, below, and on each side.\n",
    "![img/hog3.png](img/hog3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to measure how dark this pixel is compared to the pixels surrounding it, and we wanna find the direction where the biggest change happens. In this case, we can see that the pixel to the left is much lighter than this pixel, and the pixel to the right is darker than this pixel. In other words, at this exact point, the image is transitioning from a light area to a darker area. Based on that, we'll draw an arrow on top of this pixel that points from left to right. This shows the movement of lighting at this exact point.\n",
    "![img/hog4.png](img/hog4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we repeat this process for every single pixel in the image, the image turns into a map of transitions from light to dark areas. These lines are called gradients. Each gradient shows how the image flows from a light area to a dark area at that point.\n",
    "![img/hog5.png](img/hog5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now let's zoom back out. Here is what the full gradient map looks like after replacing every pixel. The gradient map is a simplified version of the original image, but it's still pretty complex. Capturing the gradient for every single pixel is more detail than we need. To detect faces, all we really need is the overall structure of the image. In other words, we can simplify this representation further.\n",
    " \n",
    " Let's take a look at one 16x16 pixel area as an example. Inside the square, on the left, we'll count up how many gradients point in each major direction. How many point up, how many point up right, how many point right, and so on. Instead of keeping track of the 256 separate gradients within this box, we'll just store a count of how many gradients point in each direction.\n",
    " \n",
    "![img/hog6.png](img/hog6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this square, the majority of gradients go from top to bottom, so that's the strongest factor that represents this area of the image. There are also gradients pointing in other directions that we'll keep track of. We'll represent those other directions here as lines that are less bold. Now, we can repeat this process for the entire image.\n",
    "\n",
    "![img/hog7.png](img/hog7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end result. The original image is now a simple representation that captures the basic structure. We can use this simplified representation to easily train a face detection model.\n",
    "\n",
    "![img/hog8.png](img/hog8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Faces in image with HOG features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn how HOG can help us build a face detection model. To train a face detection model, we'll start by collecting lots of images of different faces and converting them the HOG representations like this. These faces will be our training data.\n",
    "\n",
    "![img/hog10.png](img/hog10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use this HOG representation to train the machine learning model. We'll train the model by giving it lots of examples of HOG representations of faces so it can learn what this pattern looks like. Because faces look very obvious in HOG representations, it's pretty easy to train the machine learning algorithm to recognize those patterns. HOG face detectors can perform well with a fairly small amount of training data. Once the model is trained to recognize these kinds of face patterns, we can use it to find faces in other images.\n",
    "\n",
    "![img/hog11.png](img/hog11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's detect faces in this image. First, we'll convert this image into a HOG representation. Next, we'll use our HOG face detection model as a sliding window detector. We'll slide our face detector over each section of the image and check if it contains a face. Any place that returns true is a part of the image that contains a face.\n",
    " \n",
    "![img/hog12.png](img/hog12.png)\n",
    "![img/hog13.png](img/hog13.png)\n",
    "![img/hog14.png](img/hog14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use HOG?\n",
    "\n",
    "So why is it more effective to train the face detection model using HOG features instead of working directly with the raw images? \n",
    "\n",
    "First, HOG simplifies the image in a way that still retains the key information needed to spot faces. By simplifying the problem this way, it makes it easier for the machine learning model to solve it. But HOG has some other nice advantages, as well, that make it work better for small training sets. \n",
    "\n",
    "First, the HOG representation of an image doesn't change even when you lighten or darken the image. Since HOG only looks for changes in brightness and not absolute brightness, making an image a little brighter or a little darker doesn't change the HOG representation at all. \n",
    "\n",
    "Second, the HOG representation of an image doesn't change even if you change the shapes in the image a little bit. Because it is only looking at broad changes in the intents the over large areas of the image, small changes in shape don't matter. This is great for face detection because it means that two faces that don't look exactly the same will still have nearly the same HOG representation. \n",
    "\n",
    "\n",
    "![img/whyusehog.png](img/whyusehog.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why HOG is resistant to lighting changes?\n",
    "\n",
    "Let's look at an example. Here are two versions of the same photo. These photos are exactly the same except for one copy is brighter than the other. If we analyze the pixel values directly, these two images will have completely different pixel values. To a machine learning algorithm, they will look like two totally different images but the HOG representations for these two images are exactly the same. That means that the machine learning model needs less training data to recognize both of these images correctly.\n",
    "\n",
    "\n",
    "![img/whyusehog2.png](img/whyusehog2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-datascience",
   "language": "python",
   "name": "venv-datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
