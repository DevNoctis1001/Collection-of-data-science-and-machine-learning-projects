{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_Fine_Tuning_Pretrained_Neural_Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vMlGyStXIR_"
      },
      "source": [
        "# Image Recognition Models\n",
        "- VGG (University of Oxford) - very standard design\n",
        "- ResNet-50 (Microsoft Research) - more complex design\n",
        "- Inception v3 (Google) - even more complex design\n",
        "- MobileNet (Google) - low resource usage for mobile devices\n",
        "- NASNet (Google) - designed by algorithms\n",
        "\n",
        "## Two Uses\n",
        "- Use the **trained model directly to do image recognition**\n",
        "- **Transfer Learning**: Adapt existing model to recoginize new types of objects instead of starting from strach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2QzSjg9Xvwh"
      },
      "source": [
        "--------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8mGwX45YiOM"
      },
      "source": [
        "# 1) Using Pre-trained network for image recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6OZZU82aWMp",
        "outputId": "cebd4754-f58d-4af6-e469-21b44ca3f869"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9n_CpVFYnhW"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import  image\n",
        "from tensorflow.keras.applications import vgg16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRbyMuqlak9T"
      },
      "source": [
        "file_path = '/content/drive/MyDrive/Deeplearning_image_recognition/'"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVYlCHw7Y7Ik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0485db1-ac26-4b16-ab4d-c9c9cd1bfb69"
      },
      "source": [
        "# Load VGG 16 model that was pre-trained against ImageNet database\n",
        "model = vgg16.VGG16()\n",
        "\n",
        "# Load the image file, resizing it to 224x224 pixels (required by this VGG model)\n",
        "img = image.load_img(file_path + 'bay.jpg', target_size=(224, 224))\n",
        "\n",
        "# Convert the image to a numpy array\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "# Add a forth dimension to the image (since Keras expects a bunch of images, not a single image)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "\n",
        "# Normalize the input image's pixel values to the range used when training the neural network\n",
        "# VGG16 has built in function, so we can use it\n",
        "x = vgg16.preprocess_input(x)\n",
        "\n",
        "# Run the image through the deep neural network to make a prediction\n",
        "# the predictions we get for 1000 images, that model is trained to recognized\n",
        "predictions = model.predict(x)\n",
        "\n",
        "# Look up the names of the predicted classes. Index zero is the results for the first image.\n",
        "# but we can set the parameter top to get how many top predictions that we want to get\n",
        "predicted_classes = vgg16.decode_predictions(predictions, top=9)\n",
        "\n",
        "print(\"Top predictions for this image:\")\n",
        "\n",
        "for imagenet_id, name, likelihood in predicted_classes[0]:\n",
        "    print(\"Prediction: {} - {:2f}\".format(name, likelihood))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "40960/35363 [==================================] - 0s 0us/step\n",
            "Top predictions for this image:\n",
            "Prediction: seashore - 0.395212\n",
            "Prediction: promontory - 0.326130\n",
            "Prediction: lakeside - 0.119613\n",
            "Prediction: breakwater - 0.062801\n",
            "Prediction: sandbar - 0.045267\n",
            "Prediction: cliff - 0.011845\n",
            "Prediction: dock - 0.009196\n",
            "Prediction: boathouse - 0.003278\n",
            "Prediction: valley - 0.003194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuy0CvB-dL3q"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA06B2KKdNHH"
      },
      "source": [
        "# 2) Transfer Learning as an alternative to training a new neural network\n",
        "- using a model trained on one set of data as a starting point for modeling a new set of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfAhBFwXdTfO"
      },
      "source": [
        "To understand how transfer learning works, let's take a look at how a convolutional neural network processes an image layer by layer. A typical convolutional neural network is structured like below. The network is made up of a series of convolutional layers and the training process teaches each of those layers to be activated when it sees certain patterns in the input image. Those layers learn to tell images apart by looking for those unique patterns.\n",
        "\n",
        "![1.png](img/1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPOZ_8pyefjo"
      },
      "source": [
        "For convolution layer 1, we can see that it is looking for very basic patterns,  like splotches of color and lines in an image. \n",
        "\n",
        "![2.png](img/2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_kiaPM5fHLe"
      },
      "source": [
        "For convolution layer 2, the patterns are starting to get a little more complex.\n",
        "\n",
        "![3.png](img/3.png)\n",
        "\n",
        "![4.png](img/4.png)\n",
        "\n",
        "![5.png](img/5.png)\n",
        "\n",
        "![6.png](img/6.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6GUf6uefY30"
      },
      "source": [
        "---------\n",
        "\n",
        "**The basic idea is that neural networks learn to detect simple patterns in the top layer, and then the next layer uses that information to look for slightly more complex patterns and so on, down through all the convolutional layers. But the final layer of the neural network is a densely connected layer that uses the information from the convolutional layers to decide which object is in the image.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tpa2vbaogZmg"
      },
      "source": [
        "With transfer learning, we're gonna start with a neural network that's already been trained to recognize objects from a large dataset like ImageNet. \n",
        "- To reuse this neural network with new data, we can simply slice off the last layer. We'll keep all the layers that detect patterns, but remove the part that maps those patterns to specific objects.  We'll call this pre-trained neural network a feature extractor because we're using it to extract training features from images. \n",
        "- Next, we'll create a new neural network to replace the last layer in the original network. This is the only part that we'll have to train ourselves. \n",
        "- When we build our new image recognition system, we'll pass our new training images through the feature extractor and save the results for each training image to a file. \n",
        "- Then, we'll use those extracted features to train the new neural network. Since we're using the feature extractor to recognize shapes and patterns, our new neural network only has to learn to tell which patterns map to which objects. Since this new neural network isn't doing much work, it can learn to do it with a small amount of training data. And here's how we'll do predictions with transfer learning. \n",
        "\n",
        "![7.png](img/7.png)\n",
        "\n",
        "![8.png](img/8.png)\n",
        "\n",
        "![9.png](img/9.png)\n",
        "\n",
        "![10.png](img/10.png)\n",
        "\n",
        "![11.png](img/11.png)\n",
        "\n",
        "When we wanna test the new image, we have to first pass it through the same feature extractor. Then we can use those extracted features as input to our newly-trained neural network, which will give us the final prediction. \n",
        "\n",
        "![12.png](img/12.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX5wCTUiibOm"
      },
      "source": [
        "## 2.1) When to use Transfer Learning\n",
        "- Always try it first, because it's quick !\n",
        "- Very useful when you don't have a lot of training data but already have a model that sovles a similar problem.\n",
        "\n",
        "Training a neural network from scratch is sort of like teaching a baby to read. The baby has to learn about letters and words and sentences before it can read and understand anything. Transfer learning is more like asking an adult that already knows how to read to learn something new. Since the adult already knows how to read, they need less material to learn a new topic. They don't need alphabet flashcards and spelling tests. The same basic idea applies to neural networks. If we only have a few hundred training images for our image recognition system, we don't have enough data to teach our model from scratch, so it makes sense to start with a model trained for something else and adapt it to our problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3Azk71OlSFM"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drbjpOrrrbfc"
      },
      "source": [
        "# 3) Extracting features with a pre-trained neural network\n",
        "- we gonna use pre-trained model to train dog images.\n",
        "- then extract the features and save those features as file.\n",
        "\n",
        "## VGG16 pre-trained model creation\n",
        "- `weights`: dataset that we want to pre-trained on such as `imagenet` \n",
        "- `include_top`: `False` if we are using pre-trained model for feature extraction. False means we will chop of the last layer of neural network (in Keras terminology `top` means last layer). So by saying include_top=False, we told keras to give us neural network without the last layer attached.\n",
        "- `input_shape`: image shape of image that we want to use. If we want to use larget image size, we can bump it up here.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NKvx6bLrgVx"
      },
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications import vgg16"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVuEgX3VuCNP"
      },
      "source": [
        "# Path to folder with training data\n",
        "folder_path = '/content/drive/MyDrive/Deeplearning_image_recognition/training_data/'\n",
        "\n",
        "dog_path = Path(folder_path) / 'dogs'\n",
        "not_dog_path = Path(folder_path) / 'not_dogs'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYFAI5HRuWom"
      },
      "source": [
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Load all the not dogs images\n",
        "for i in not_dog_path.glob('*.png'):\n",
        "  # load the image from disk\n",
        "  img = image.load_img(i)\n",
        "\n",
        "  # convert image into a numpy array\n",
        "  img_array = image.img_to_array(img)\n",
        "\n",
        "  # add the image to the list of images\n",
        "  images.append(img_array)\n",
        "\n",
        "  # for each 'not dog' image, the expected value should be 0\n",
        "  labels.append(0)\n",
        "\n",
        "# Load all dogs images\n",
        "for i in dog_path.glob('*.png'):\n",
        "  # load the image from disk\n",
        "  img = image.load_img(i)\n",
        "\n",
        "  # convert image to a numpy array\n",
        "  img_array = image.img_to_array(img)\n",
        "\n",
        "  # add the image to the list of images\n",
        "  images.append(img_array)\n",
        "\n",
        "  # for each 'dog' image, the expected value should be 1\n",
        "  labels.append(1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl4IDGDmvfAg",
        "outputId": "c1c28b29-d86b-4b87-fb59-987aea085a77"
      },
      "source": [
        "# Create a single numpy array with all the images we loaded\n",
        "# Keras expected numpy array instead of normal python list\n",
        "x_train = np.array(images)\n",
        "\n",
        "# also Convert the labels to a numpy array\n",
        "y_train = np.array(labels)\n",
        "\n",
        "# Normalize image data to 0-1 range\n",
        "x_train = vgg16.preprocess_input(x_train)\n",
        "\n",
        "# Load a pre-trained neural network to use as a feature extractor\n",
        "pretrained_nn = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
        "\n",
        "# Extract features for each image (all in one pass)\n",
        "features_x = pretrained_nn.predict(x_train)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALBZdbjdzAYV",
        "outputId": "006394f3-2ea1-47ae-8565-4fca16c605fe"
      },
      "source": [
        "# Save the array of extracted features to a file\n",
        "joblib.dump(features_x, 'x_train.dat')\n",
        "\n",
        "# Save the matching array of expected values to a file\n",
        "joblib.dump(y_train, 'y_train.dat')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['y_train.dat']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejHI2mMGzQt8"
      },
      "source": [
        "--------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCJmKlOizrIw"
      },
      "source": [
        "# 4) Training a new neural network with extracted features\n",
        "For the transfer learning, in our case we used VGG16 to extract features.  As a result,\n",
        "- when loading data, we don't need to load Raw Data. Instead we will load **extracted features of X and y(labels)**.\n",
        "- when we create a neural network, we don't need to use Convolution Layer anymore. **We only need to create the final Dense layer of the network and need to retrain.**\n",
        "- As our problem is to predict dog or not (classificaiton problem), we will use binary_crossentropy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGioFRdXz0eq"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "\n",
        "from pathlib import Path\n",
        "import joblib"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiS_Wyiz0oVp"
      },
      "source": [
        "# Load data set\n",
        "# Here instead of loading \n",
        "x_train = joblib.load('x_train.dat')\n",
        "y_train = joblib.load('y_train.dat')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_KPwwly1VNt",
        "outputId": "14df4933-ddf4-4189-df86-4f3bc7b8b2ed"
      },
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((58, 2, 2, 512), (58,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkjkYmHI1ctk",
        "outputId": "f3c6dad0-cc7a-43ba-e155-631b1d0bec9e"
      },
      "source": [
        "x_train.shape[1:]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 2, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a94qYOOe0-t1"
      },
      "source": [
        "# Create a model and add layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Flatten(input_shape=x_train.shape[1:]))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss = 'binary_crossentropy',\n",
        "    optimizer = 'adam',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vEYKW9_2RrW",
        "outputId": "3112d4d9-1343-488e-b224-1926ba845a8f"
      },
      "source": [
        "# Train the model\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2/2 [==============================] - 1s 7ms/step - loss: 15.3690 - accuracy: 0.4698\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 2.5370 - accuracy: 0.8560\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.0924 - accuracy: 0.9781\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 1.0083 - accuracy: 0.9447\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 3.2212e-09 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 5.1879e-12 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 2.7423e-05 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 4.7887e-07 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 1.0432e-10 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 5.3417e-11 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fafc806f890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMR4iM4m21vn"
      },
      "source": [
        "# Save the neural network structure\n",
        "model_structure = model.to_json()\n",
        "f = Path('transfer_learning_model_structure.json')\n",
        "f.write_text(model_structure)\n",
        "\n",
        "# Save neural network's trained weights\n",
        "model.save_weights('transfer_learning_model_weights.h5')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvIdPaep3PU_"
      },
      "source": [
        "\n",
        "-------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prHjQTSv31Pl"
      },
      "source": [
        "# 5) Making predictions with transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU56XBy-3_17"
      },
      "source": [
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSdVkNaM3FPS"
      },
      "source": [
        "# Load json file that contains model's structure\n",
        "f = Path('transfer_learning_model_structure.json')\n",
        "model_structure = f.read_text()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b33vCd3A3Us8"
      },
      "source": [
        "# Recreate keras model from json structure\n",
        "model = model_from_json(model_structure)\n",
        "\n",
        "# Reload model's trained weights\n",
        "model.load_weights('transfer_learning_model_weights.h5')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6NNAMp-3mAC"
      },
      "source": [
        "# Load the image file to test, resize to 64x64 pixels (required by this model)\n",
        "img = image.load_img(file_path + 'dog.png', target_size=(64, 64, 3))\n",
        "\n",
        "# Convert image to numpy array\n",
        "img = image.img_to_array(img)\n",
        "\n",
        "# Add fourth dimension to the image (since Keras expects a bunch of images, not as single one)\n",
        "images = np.expand_dims(img, axis=0)\n",
        "\n",
        "# Normalize the data\n",
        "images = vgg16.preprocess_input(images)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHH_yVnS4Y6J",
        "outputId": "d106a101-25c0-4e58-f349-4f343b6df625"
      },
      "source": [
        "# Use the pre-trained neural network to extract features from our test image (the same way we did to train the model)\n",
        "# Basically get the features of test image so that we can pass it to the model for final prediciton\n",
        "feature_extraction_model = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
        "features = feature_extraction_model.predict(images)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:8 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7faf3e21b5f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktj7qtce5UW-"
      },
      "source": [
        "# Given the extracted features, make a final prediction using our own model\n",
        "results = model.predict(features)\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwK9C0T55b_r",
        "outputId": "a631735e-39a6-45e5-e40e-e94307c0f098"
      },
      "source": [
        "# Since we are only testing one image with possible class, we only need to check the first result's first element\n",
        "single_result = results[0][0]\n",
        "\n",
        "# Print the result\n",
        "print(\"Likelihood that this image contains a dog: {}%\".format(int(single_result * 100)))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Likelihood that this image contains a dog: 100%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}